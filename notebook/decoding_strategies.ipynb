{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating Different Decoding Strategies with Hugging Face Transformers\n",
    "In this notebook, we will explore various decoding strategies for text generation using a small encoder-decoder model from Hugging Face's Transformers library. We'll apply these strategies to two different datasets:\n",
    "\n",
    "- Translation task where deterministic strategies are expected to perform better.\n",
    "- Summarization task where stochastic strategies might yield more diverse and informative outputs.\n",
    "\n",
    "The decoding strategies we'll test include:\n",
    "\n",
    "1. Greedy Search\n",
    "2. Beam Search\n",
    "3. Temperature Sampling\n",
    "4. Top-k Sampling\n",
    "5. Top-p (Nucleus) Sampling\n",
    "\n",
    "We'll define a custom ```generate_text``` function to apply these strategies and evaluate their performance using appropriate metrics for each dataset.\n",
    "\n",
    "Here are some useful links you might want to check:\n",
    "- [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- [transformers\\AutoModel](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)\n",
    "- [transformers\\AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n",
    "- [Google's T5](https://arxiv.org/pdf/1910.10683)\n",
    "- [T5-small](https://huggingface.co/google-t5/t5-small)\n",
    "- [Flan-T5-small](https://huggingface.co/google/flan-t5-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets sacrebleu rouge_score evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlosgonzalezv/ICAI/4IMAT/NLP.II/P3-NLPII/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Pre-trained Model and Tokenizer\n",
    "\n",
    "We'll use the ```flan-T5-small``` model, which is a small encoder-decoder model suitable for both story_gen and summarization tasks. This model is based on Google's ```t5-small``` model, but fine-tuned on more than 1000 additional tasks covering also more languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlosgonzalezv/ICAI/4IMAT/NLP.II/P3-NLPII/lib/python3.10/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-small'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, strategy='greedy', max_length=50, **kwargs):\n",
    "    \"\"\"Generates text based on the specified decoding strategy.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text to be processed by the model.\n",
    "        strategy (str, optional): The decoding strategy to use. Defaults to 'greedy'.\n",
    "            Options include:\n",
    "            - 'greedy': Greedy search decoding.\n",
    "            - 'beam': Beam search decoding.\n",
    "            - 'temperature': Temperature sampling.\n",
    "            - 'top-k': Top-k sampling.\n",
    "            - 'top-p': Top-p (nucleus) sampling.\n",
    "            - 'contrastive': Contrastive search decoding.\n",
    "        max_length (int, optional): The maximum length of the generated text. Defaults to 50.\n",
    "        **kwargs: Additional keyword arguments specific to the decoding strategy.\n",
    "\n",
    "    Keyword Args:\n",
    "        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n",
    "            Used when `strategy='beam'`.\n",
    "        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n",
    "            Used when `strategy='temperature'`.\n",
    "        top_k (int, optional): The number of highest probability vocabulary tokens to keep for top-k filtering. Defaults to 50.\n",
    "            Used when `strategy='top-k'` or when `strategy='contrastive'`.\n",
    "        top_p (float, optional): Cumulative probability for nucleus sampling. Defaults to 0.95.\n",
    "            Used when `strategy='top-p'`.\n",
    "        penalty_alpha (float, optional): Contrastive search penalty factor. Defaults to 0.6.\n",
    "            Used when `strategy='contrastive'`.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text based on the decoding strategy.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unknown decoding strategy is specified.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if strategy == 'greedy':\n",
    "            output_ids = model.generate(input_ids, max_length=max_length)\n",
    "        elif strategy == 'beam':\n",
    "            num_beams = kwargs.get('num_beams', 5)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, num_beams=num_beams, early_stopping=True\n",
    "            )\n",
    "        elif strategy == 'temperature':\n",
    "            temperature = kwargs.get('temperature', 1.0)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, temperature=temperature\n",
    "            )\n",
    "        elif strategy == 'top-k':\n",
    "            top_k = kwargs.get('top_k', 50)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_k=top_k\n",
    "            )\n",
    "        elif strategy == 'top-p':\n",
    "            top_p = kwargs.get('top_p', 0.95)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_p=top_p\n",
    "            )\n",
    "        elif strategy == 'contrastive':\n",
    "            penalty_alpha = kwargs.get('penalty_alpha', 0.6)\n",
    "            top_k = kwargs.get('top_k', 4)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, penalty_alpha=penalty_alpha, top_k=top_k\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown strategy: {}\".format(strategy))\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 1: Neural Machine Translation\n",
    "The WMT16 English-German dataset is a collection of parallel sentences in English and German used for machine translation tasks. It is part of the Conference on Machine story_gen (WMT) shared tasks, which are benchmarks for evaluating machine story_gen systems. The dataset contains professionally translated sentences and covers a variety of topics, making it ideal for training and evaluating story_gen models.\n",
    "\n",
    "We'll use a subset (1% of the test split) of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_nmt = load_dataset('wmt16', 'de-en', split='test[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "Lamb war es wichtig zu betonen, dass sein \"süßer Hund\" aber noch lebe und wahrscheinlich aufgeregt sei, und er sagte, die Familienkontakte der toten Frau könnten auf dem Handy gefunden werden.\n",
      "\n",
      "Target Text:\n",
      "Lamb made a point to say his \"sweet dog\" was there alive and probably upset, and said the dead woman's family contacts could be found on her phone.\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_nmt['translation'])\n",
    "input_text = sample['de']\n",
    "target_text = sample['en']\n",
    "\n",
    "print(\"Input Text:\")\n",
    "print(input_text)\n",
    "print(\"\\nTarget Text:\")\n",
    "print(target_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the dataset to prepare it for input into the T5 model. The T5 model expects input in a specific format, including a task prefix. This is because the T5 model is a multi-purpose model, being able to perform several task, so we need to tell it what it must do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_translation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the translation task.\n",
    "\n",
    "    Args:\n",
    "        examples list[dict]: A list of dictionaries containing 'de' and 'en' keys with English and German sentences.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of lists with added 'src_texts' and 'tgt_texts' keys for model input and target.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'translate German to English: ' to the German sentence.\n",
    "    - Stores the result in 'src_texts'.\n",
    "    - Copies the English sentence to 'tgt_texts'.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a \"translate German to English: \" prefix\n",
    "    texts['src_texts'] = ['translate German to English: ' + ex['de'] for ex in examples]\n",
    "    texts['tgt_texts'] = [ex['en'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_nmt_preproc = preprocess_translation(dataset_nmt[\"translation\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can generate a translation using the t5 model. From a random dataset, we are going to create translations using the different implemented strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate German to English: Die Rede war mit Obama nicht abgesprochen, ein Treffen hatte dieser mit Hinweis auf die seinerzeit bevorstehende Wahl in Israel abgelehnt.\n",
      "Original translation: \n",
      "The speech had not been agreed with Obama, who had rejected a meeting with reference to the election that was at that time impending in Israel.\n",
      "Greedy Search: \n",
      "The Rede was not mentioned, a meeting had rejected this with reference to the previous election in Israel.\n",
      "Beam Search: \n",
      "The Rede was not mentioned, a meeting had rejected this with reference to the forthcoming elections in Israel.\n",
      "Temperature Sampling: \n",
      "The White House declined to comment on the reaction to the Rede with Obama, a meeting had rejected this with reference to the future Palestinian elections.\n",
      "Top-k Sampling: \n",
      "The Rede and Obama did not respond to both. A meeting by iin was rejected with note on its future election in Israel.\n",
      "Top-p Sampling: \n",
      "The resurgence had not been an issue to Obama, an effort to rewrite the judgment on its future in Israel by pointing out that, from that perspective, there would be no consensus of opinion arising from such a\n",
      "Contrastive Search: \n",
      "The Rede was not a member of the Obama administration, a meeting had rejected this with reference to the current election in Israel.\n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_nmt_preproc['src_texts']))\n",
    "random_src_sentence = dataset_nmt_preproc['src_texts'][random_index]\n",
    "random_tgt_sentence = dataset_nmt_preproc['tgt_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "translation_greedy= generate_text(random_src_sentence, strategy='greedy')\n",
    "translation_beam_search = generate_text(random_src_sentence, strategy='beam', num_beams=5)\n",
    "translation_temperature = generate_text(random_src_sentence, strategy='temperature', temperature=0.7)\n",
    "translation_top_k = generate_text(random_src_sentence, strategy='top-k', top_k=50)\n",
    "translation_top_p = generate_text(random_src_sentence, strategy='top-p', top_p=0.95)\n",
    "translation_contrastive = generate_text(random_src_sentence, strategy='contrastive', top_k=4, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Original translation: \\n{random_tgt_sentence}\")\n",
    "print(f\"Greedy Search: \\n{translation_greedy}\")\n",
    "print(f\"Beam Search: \\n{translation_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{translation_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{translation_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{translation_top_p}\")\n",
    "print(f\"Contrastive Search: \\n{translation_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do you see something different between the deterministic and the stochastic strategies? Try different random sentences.\n",
    ">>> Sí, las estrategias deterministas (greedy, beam, contrastive) generan salidas más estables y repetidas que las estocásticas. Por otro lado, las estocásticas producen respuestas más diversas y creativas, lo que suele significar más errores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric: BLEU Score\n",
    "#### What is BLEU Score?\n",
    "The BLEU (Bilingual Evaluation Understudy) score is a metric for evaluating the quality of text that has been machine-translated from one language to another. It compares a candidate translation to one or more reference translations and calculates a score based on the overlap of n-grams (contiguous sequences of words).\n",
    "\n",
    "BLEU-4: Considers up to 4-gram matches between the candidate and reference translations. It provides a balance between precision (matching words) and fluency (maintaining the structure of the language). Using BLEU-4 allows us to capture not just individual word matches (unigrams) but also phrases of up to four words. This makes the evaluation more sensitive to the quality of the translation in terms of both accuracy and fluency.\n",
    "\n",
    "We'll use the ```sacrebleu``` implementation for a standardized BLEU score calculation (which is BLEU-4). You can check the details [here](https://aclanthology.org/W14-3346.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating strategy: greedy\n",
      "BLEU-4 score for greedy: 20.99\n",
      "Evaluating strategy: beam\n",
      "BLEU-4 score for beam: 21.14\n",
      "Evaluating strategy: temperature\n",
      "BLEU-4 score for temperature: 9.75\n",
      "Evaluating strategy: top-k\n",
      "BLEU-4 score for top-k: 12.33\n",
      "Evaluating strategy: top-p\n",
      "BLEU-4 score for top-p: 13.77\n",
      "Evaluating strategy: contrastive\n",
      "BLEU-4 score for contrastive: 19.34\n"
     ]
    }
   ],
   "source": [
    "strategies = ['greedy', 'beam', 'temperature', 'top-k', 'top-p', 'contrastive']\n",
    "results = {}\n",
    "\n",
    "hyperparameters = {\n",
    "    'num_beams': 5,\n",
    "    'temperature': 1.0,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.95,\n",
    "    'penalty_alpha': 0.6\n",
    "}\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"Evaluating strategy: {strategy}\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for src_text, tgt_text in zip(dataset_nmt_preproc[\"src_texts\"], dataset_nmt_preproc[\"tgt_texts\"]): \n",
    "        pred = generate_text(src_text, strategy=strategy, **hyperparameters)\n",
    "        predictions.append(pred)\n",
    "        references.append([tgt_text])  # SacreBLEU expects a list of references\n",
    "\n",
    "    # Compute BLEU-4 score\n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')\n",
    "    results[strategy] = bleu['score']\n",
    "    print(f\"BLEU-4 score for {strategy}: {bleu['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above translations and the BLEU score of the different strategies, which strategy would you choose for this use case?\n",
    ">>> Eligiría la estrategia beam search, ya que en este caso es la que tiene mayor BLEU-4 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 2: Story generation\n",
    "\n",
    "The ```WritingPrompts``` dataset is a collection of imaginative prompts and corresponding stories from the Reddit community. It contains over 300,000 stories written in response to various prompts, making it suitable for training and evaluating models on creative text generation tasks.\n",
    "\n",
    "We'll use a subset of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_st_gen = load_dataset('llm-aes/writing-prompts', split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:\n",
      " Write a -punk story with an unusual theme ( i.e . Teapunk , Catpunk , Solarpunk )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_st_gen['prompt'])\n",
    "\n",
    "print(\"Sample Text:\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to tell the t5 model to generate text after the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_story_generation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the story generation task.\n",
    "\n",
    "    Args:\n",
    "        examples (list[dict]): A list of dictionaries containing 'prompt' key.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of list with added 'src_texts' for model input.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'Write a story based on: ' to the prompt.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a task prefix\n",
    "    texts['src_texts'] = ['Write a story based on: ' + ex['prompt'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_st_gen_preproc = preprocess_story_generation(dataset_st_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have no original story to compare how good the model generates a story, you should compare the different decoding strategies by looking at some random stories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a story based on:  007 is dead and today MI6 got a new recruit for the position and his name is Paul Blart\n",
      "\n",
      "Greedy Search: \n",
      "Paul Blart, a former MI6 player, was a member of the team that won the senate in the first round of the 2015 season. He was a member of the team that won the senate in the second round of the 2015 season. He was a member of the team that won the senate in the first round of the 2015 season. He was a member of the team that won the senate in the second round of the 2015 season. He was a member of the team that won the senate in the third round of the 2015 season. He was a member of the team that won the senate in the fourth round of the 2015 season. He was a member of the team that won the senate in the fourth round of the 2015 season. He was a member of the team that won the senate in the fourth round of the 2015 season. He was a member of the team that won the senate in the fourth round of the 2015 season. He was a member of the team that won the senate in the fourth round of the 2015 season. He was a member of the team that won the senate in the fourth round of the 2015 season. He was a member of the team that won the senate in the fourth round of the 2015 season.\n",
      "Beam Search: \n",
      "Paul Blart is dead and today MI6 got a new recruit for the position and his name is Paul Blart. He is a former professional footballer who played as a midfielder in the national football league. Blart is a former professional footballer who played as a midfielder in the national football league. Blart is a former professional footballer who played as a midfielder in the national football league. Blart is a former professional footballer who played as a midfielder in the national football league. Blart is a former professional footballer who played as a midfielder in the national football league.\n",
      "Temperature Sampling: \n",
      "007 was a member of the squad for a summer campaign in the asian town of ngeln. He was also a member of the squad and he was a candidate for the position. 007 was one of the few senior teams that had been able to get a position in the squad. He was injured in the training, and he was found dead.\n",
      "Top-k Sampling: \n",
      "07:00 is tomorrow morning, with a squad of 50 members. With the arrival of 07:00, the team will meet a number of former high school football players. In order to meet the 'em, the squad will have to prepare a squad of six players that will face MI6 and the squad for that position. The new recruits, a former world figure who was playing for their hometown club 007, were 'gongs and peasants'day after the match after the 1-0 defeat with 07:00 and 08:00 on Wednesday. \"I don't know if it will be the long dream of me and if there will be some excitement there,\" said a spokesman for the squad. When they are on the pitch, he is now undergoing a drease-of-three treatment and will be seen as a member of the national team. MSG manager Yuri Khomeini joined the squad this afternoon.\n",
      "Top-p Sampling: \n",
      "Paul Blart was driving for his wife when the funeral came. The family's dad, now a former doctor, says he had a \"four bourbon remittance\" that was over after his death. However, during the funeral, the family said the post was due to be returned to the police and he is dead. 007 now looks at him and his parents. He is only 84, and his older brother - whom he has a crush on - is 61. The funeral of the funeral ended on.\n",
      "Contrastive Search: \n",
      "Paul Blart, a former scout who played for the scouts and scouts in the scouts and scouts, was a regular at the club. He was a very good player and he was a great player. ''He was a great player and he was a great player.'' ''He was a great player.\n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_st_gen_preproc['src_texts']))\n",
    "random_src_sentence = dataset_st_gen_preproc['src_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "story_gen_greedy= generate_text(random_src_sentence, max_length=300, strategy='greedy')\n",
    "story_gen_beam_search = generate_text(random_src_sentence, max_length=300, strategy='beam', num_beams=5)\n",
    "story_gen_temperature = generate_text(random_src_sentence, max_length=300, strategy='temperature', temperature=0.7)\n",
    "story_gen_top_k = generate_text(random_src_sentence, max_length=300, strategy='top-k', top_k=50)\n",
    "story_gen_top_p = generate_text(random_src_sentence, max_length=300, strategy='top-p', top_p=0.95)\n",
    "story_gen_contrastive = generate_text(random_src_sentence, max_length=300, strategy='contrastive', top_k=4, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Greedy Search: \\n{story_gen_greedy}\")\n",
    "print(f\"Beam Search: \\n{story_gen_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{story_gen_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{story_gen_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{story_gen_top_p}\")\n",
    "print(f\"Contrastive Search: \\n{story_gen_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above generated stories of the different strategies, which strategy would you choose for this use case?\n",
    ">>> Eligiría un método estocástico ya que introduce más creatividad, en este caso concreto, eligiría top-p sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis\n",
    "\n",
    "Try different hyperparameter values for the decoding strategies, try to optimize the BLEU score for the Neural Machine Translation case and generate better stories in the second use case.\n",
    "\n",
    "- Which optimal configuration have you found for use case 1? Which are your conclusions based on your analysis?\n",
    ">>> beam: BLEU=21.94, config={'num_beams': 3}, es la opción óptima. Las estrategias de muestreo tienen menos score ya que introducen más variabilidad y son menos literales. Sin embargo, el método contrastive, con estos resultados: contrastive: BLEU=20.47, config={'top_k': 8, 'penalty_alpha': 0.4}, nos podría llegar a ser útil si queremos reducir la repetición de las estrategias deterministas sin llegar a perder mucha fidelidad.\n",
    "\n",
    "- Which optimal configuration have you found for use case 2? Which are your conclusions bsaed on your analysis?\n",
    ">>> Estrategia top-p con parámetros : {top_p: 0.9, temperature: 0.7, max_length = 200-300, repetition_penalty= 1.2}. Esto se debe a que para generar historias es más útil que el modelo introduzca creatividad sin perder la coherencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search for translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy: BLEU=20.99, config={}\n",
      "beam: BLEU=21.94, config={'num_beams': 3}\n",
      "temperature: BLEU=18.57, config={'temperature': 0.5}\n",
      "top-k: BLEU=12.16, config={'top_k': 40}\n",
      "top-p: BLEU=14.80, config={'top_p': 0.8, 'temperature': 1.0}\n",
      "contrastive: BLEU=20.47, config={'top_k': 8, 'penalty_alpha': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Mini grid-search para decoding strategies\n",
    "def grid_search_decoding(src_texts, tgt_texts, grids, strategies=None, n_samples=200):\n",
    "    strategies = strategies or list(grids.keys())\n",
    "    n = min(n_samples, len(src_texts))\n",
    "    # submuestreo aleatorio para rapidez\n",
    "    idxs = random.sample(range(len(src_texts)), n)\n",
    "    src_subset = [src_texts[i] for i in idxs]\n",
    "    tgt_subset = [tgt_texts[i] for i in idxs]\n",
    "\n",
    "    best_results = {}\n",
    "    for strategy in strategies:\n",
    "        param_grid = grids.get(strategy, {})\n",
    "        if not param_grid:\n",
    "            # estrategia sin hiperparámetros\n",
    "            preds = [generate_text(s, strategy=strategy) for s in src_subset]\n",
    "            bleu = bleu_metric.compute(predictions=preds, references=[[t] for t in tgt_subset], smooth_method='exp')\n",
    "            best_results[strategy] = {'best_score': bleu['score'], 'best_config': {}}\n",
    "            continue\n",
    "\n",
    "        keys, values = zip(*param_grid.items())\n",
    "        combos = list(itertools.product(*values))\n",
    "        best_score, best_config = -1.0, None\n",
    "\n",
    "        for combo in tqdm(combos, desc=f\"Search {strategy}\", leave=False):\n",
    "            params = dict(zip(keys, combo))\n",
    "            preds = [generate_text(s, strategy=strategy, **params) for s in src_subset]\n",
    "            bleu = bleu_metric.compute(predictions=preds, references=[[t] for t in tgt_subset], smooth_method='exp')\n",
    "            score = bleu['score']\n",
    "            if score > best_score:\n",
    "                best_score, best_config = score, params.copy()\n",
    "\n",
    "        best_results[strategy] = {'best_score': best_score, 'best_config': best_config}\n",
    "    return best_results\n",
    "\n",
    "# Definir grids\n",
    "grids = {\n",
    "    'greedy': {},\n",
    "    'beam': {'num_beams': [3, 5, 8, 10]},\n",
    "    'temperature': {'temperature': [0.5, 0.7, 1.0, 1.2]},\n",
    "    'top-k': {'top_k': [10, 40, 80]},\n",
    "    'top-p': {'top_p': [0.8, 0.9, 0.95], 'temperature': [0.7, 1.0]},\n",
    "    'contrastive': {'top_k': [4, 8], 'penalty_alpha': [0.4, 0.6, 0.8]}\n",
    "}\n",
    "\n",
    "src_texts = dataset_nmt_preproc['src_texts']\n",
    "tgt_texts = dataset_nmt_preproc['tgt_texts']\n",
    "\n",
    "results = grid_search_decoding(src_texts, tgt_texts, grids, n_samples=200)\n",
    "for strat, info in results.items():\n",
    "    print(f\"{strat}: BLEU={info['best_score']:.2f}, config={info['best_config']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P3-NLPII",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
